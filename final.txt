import argparse
import sys
import logging
import re
from collections import defaultdict, deque
import cv2
import numpy as np
import easyocr
from skimage import exposure
import supervision as sv
from ultralytics import YOLO
import time  # added for inference time measurement

# -------------------------------------------------------------------
#                        Kalman + Utilities
# -------------------------------------------------------------------
class ViewTransformer:
    def __init__(self, source: np.ndarray, target: np.ndarray) -> None:
        self.m = cv2.getPerspectiveTransform(source.astype(np.float32), target.astype(np.float32))

    def transform_points(self, points: np.ndarray) -> np.ndarray:
        if points.size == 0:
            return points
        reshaped_points = points.reshape(-1, 1, 2).astype(np.float32)
        transformed_points = cv2.perspectiveTransform(reshaped_points, self.m)
        return transformed_points.reshape(-1, 2)

class SimpleKalmanFilter:
    def __init__(self, initial_estimate: float, process_variance: float = 0.1, measurement_variance: float = 1.0):
        self.estimate = initial_estimate
        self.error_covariance = 1.0
        self.process_variance = process_variance
        self.measurement_variance = measurement_variance

    def update(self, measurement: float) -> float:
        self.error_covariance += self.process_variance
        K = self.error_covariance / (self.error_covariance + self.measurement_variance)
        self.estimate = self.estimate + K * (measurement - self.estimate)
        self.error_covariance = (1 - K) * self.error_covariance
        return self.estimate
def convert_speed(speed_mps, unit='mph'):
    conversions = {'mph': 2.23694, 'kmh': 3.6, 'mps': 1.0}
    return speed_mps * conversions.get(unit, 1.0)

# -------------------------------------------------------------------
#                    Perspective Transform
# -------------------------------------------------------------------

# -------------------------------------------------------------------
#                    Speed Estimator
# -------------------------------------------------------------------
class SpeedEstimator:
    def __init__(self, fps, window_size=5):
        
        self.fps = fps
        self.window_size = window_size
        self.positions = defaultdict(lambda: deque(maxlen=int(fps * 3)))  # in pixel coordinates
        self.speed_filters = {}
        self.last_speeds = defaultdict(float)
        self.real_world_positions = defaultdict(lambda: deque(maxlen=int(fps * 3)))  # if available

    def add_position(self, tracker_id, point, real_world_point=None):
        
        self.positions[tracker_id].append(point)
        if real_world_point is not None:
            self.real_world_positions[tracker_id].append(real_world_point)
        if len(self.positions[tracker_id]) >= 2:
            return self.calculate_speed(tracker_id)
        return 0.0

    def calculate_speed(self, tracker_id):
    
        # Decide which position list (real-world or pixel) to use
        if tracker_id in self.real_world_positions and len(self.real_world_positions[tracker_id]) >= self.window_size:
            pts = np.array(self.real_world_positions[tracker_id])
            use_real_world = True
        else:
            pts = np.array(self.positions[tracker_id])
            use_real_world = False

        # If we have fewer than 'window_size' points, skip speed calculation
        if pts.shape[0] < self.window_size:
            # Either return the last known speed or return 0.0 to show "no data" yet
            return self.last_speeds.get(tracker_id, 0.0)

        # Now that we have enough points, proceed with your calculations
        recent_speeds = []

        # 1. Regression-based speed calculation (across the last window_size frames)
        t = np.arange(self.window_size)  # indices 0 .. (window_size-1)
        window_pts = pts[-self.window_size:]
        slope_x, _ = np.polyfit(t, window_pts[:, 0], 1)
        slope_y, _ = np.polyfit(t, window_pts[:, 1], 1)
        reg_speed_pixels = np.sqrt(slope_x**2 + slope_y**2) * self.fps  # in pixels/second
        reg_speed_mps = reg_speed_pixels  # (no pixel->meter unless you incorporate that externally)
        recent_speeds.append(reg_speed_mps)

        # 2. Windowed instantaneous speed calculation
        displacement_pixels = np.linalg.norm(pts[-1] - pts[-self.window_size])
        dt = (self.window_size - 1) / self.fps  # time interval for the window
        inst_speed_pixels = displacement_pixels / dt
        if not use_real_world:
            # In case you have a pixel-to-meter factor; otherwise keep as-is
            inst_speed_mps = inst_speed_pixels  # or inst_speed_pixels * self.pixel_to_meter
        else:
            inst_speed_mps = inst_speed_pixels
        recent_speeds.append(inst_speed_mps)

        # Average both speed estimates
        measured_speed_mps = np.mean(recent_speeds)

        # Apply Kalman Filtering for smoothing
        if tracker_id not in self.speed_filters:
            self.speed_filters[tracker_id] = SimpleKalmanFilter(
                initial_estimate=measured_speed_mps,
                process_variance=0.03,      # tune as needed
                measurement_variance=0.15   # tune as needed
            )
        filtered_speed_mps = self.speed_filters[tracker_id].update(measured_speed_mps)

        # Convert to mph (or desired unit) and store
        self.last_speeds[tracker_id] = convert_speed(filtered_speed_mps, 'mph')

        return self.last_speeds[tracker_id]


MORPH_KERNEL = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))

def extract_plate(image, box, margin=0.2):
    try:
        x1, y1, x2, y2 = map(int, box)
        h, w = image.shape[:2]
        box_w, box_h = x2 - x1, y2 - y1
        aspect_ratio = box_w / box_h if box_h != 0 else 0
        if 2.0 < aspect_ratio < 6.0:
            margin *= 1.2
        dx = int(margin * box_w)
        dy = int(margin * box_h)
        x1e, y1e = max(0, x1 - dx), max(0, y1 - dy)
        x2e, y2e = min(w, x2 + dx), min(h, y2 + dy)
        crop = image[y1e:y2e, x1e:x2e]
        return crop
    except Exception as e:
        logging.error(f"Plate extraction failed: {e}")
        return None

def denoise_image(image):
    return cv2.fastNlMeansDenoisingColored(image, None, 10, 10, 7, 21)

def adjust_gamma(image):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    brightness = np.mean(gray)
    gamma = 0.8 if brightness < 100 else 1.2 if brightness > 150 else 1.0
    return exposure.adjust_gamma(image, gamma)

def enhance_image_colors(image):
    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
    l, a, b = cv2.split(lab_image)
    clahe = cv2.createCLAHE(clipLimit=2.5, tileGridSize=(8,8))
    l = clahe.apply(l)
    a = cv2.equalizeHist(a)
    b = cv2.equalizeHist(b)
    lab_image = cv2.merge([l, a, b])
    return cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)

def remove_white_pixels(image):
    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
    lower_white = np.array([0, 0, 180], dtype=np.uint8)
    upper_white = np.array([180, 50, 255], dtype=np.uint8)
    white_mask = cv2.inRange(hsv, lower_white, upper_white)
    non_white_mask = cv2.bitwise_not(white_mask)
    filtered_image = cv2.bitwise_and(image, image, mask=non_white_mask)
    kernel = np.ones((3,3), np.uint8)
    return cv2.morphologyEx(filtered_image, cv2.MORPH_CLOSE, kernel)

def adaptive_thresholding(image):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    bilateral = cv2.bilateralFilter(gray, 9, 75, 75)
    thresh = cv2.adaptiveThreshold(
        bilateral, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
        cv2.THRESH_BINARY_INV, 19, 9
    )
    kernel = np.ones((3,3), np.uint8)
    cleaned = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)
    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(cleaned, connectivity=8)
    height, width = cleaned.shape
    center_y, center_x = height // 2, width // 2
    min_size = 100
    max_distance = width // 4
    result = np.zeros_like(cleaned)
    for i in range(1, num_labels):
        size = stats[i, cv2.CC_STAT_AREA]
        cx, cy = centroids[i]
        distance = np.sqrt((cx - center_x)**2 + (cy - center_y)**2)
        if size > min_size and distance < max_distance:
            result[labels == i] = 255
    return result

def sharpen_image(image):
    kernel = np.array([[0, -1,  0],
                       [-1,  5, -1],
                       [0, -1,  0]], dtype=np.float32)
    return cv2.filter2D(image, -1, kernel)

def fill_black_holes(binary_image):
    if len(binary_image.shape) == 3:
        binary_image = cv2.cvtColor(binary_image, cv2.COLOR_BGR2GRAY)
    _, binary = cv2.threshold(binary_image, 127, 255, cv2.THRESH_BINARY)
    kernel = np.ones((3, 3), np.uint8)
    closed = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
    dilated = cv2.dilate(closed, kernel, iterations=0)
    return dilated

def full_preprocess_pipeline(plate_image):
    plate_image = denoise_image(plate_image)
    plate_image = adjust_gamma(plate_image)
    plate_image = enhance_image_colors(plate_image)
    plate_image = remove_white_pixels(plate_image)
    plate_image = adaptive_thresholding(plate_image)
    plate_image = sharpen_image(plate_image)
    plate_image = fill_black_holes(plate_image)
    return plate_image

def quick_precheck(plate_crop, precheck_white_ratio=0.1):
    gray = cv2.cvtColor(plate_crop, cv2.COLOR_BGR2GRAY)
    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    white_pixels = cv2.countNonZero(binary)
    total_pixels = binary.shape[0] * binary.shape[1]
    ratio = white_pixels / total_pixels
    return ratio > precheck_white_ratio

def find_frequent_element(li):
    count = {}
    mode = None
    max_count = 0
    for item in li:
        count[item] = count.get(item, 0) + 1
        if count[item] > max_count:
            max_count = count[item]
            mode = item
    return mode if max_count >= 3 else 0

def find_base_angle(image):
    if len(image.shape) == 2 or image.shape[2] == 1:
        gray = image
    else:
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    _, binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)
    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if not contours:
        return 0.0
    largest_contour = max(contours, key=cv2.contourArea)
    rect = cv2.minAreaRect(largest_contour)
    angle = rect[-1]
    if angle < -45:
        angle = 90 + angle
    else:
        angle = -angle
    return angle

def rotate_image(image, angle):
    (h, w) = image.shape[:2]
    center = (w // 2, h // 2)
    M = cv2.getRotationMatrix2D(center, angle, 1.0)
    rotated = cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)
    return rotated

def detect_two_digit_numbers(image, reader, pattern):
    results = reader.readtext(image, detail=1, paragraph=False, decoder='greedy')
    valid_texts = []
    for (bbox, text, prob) in results:
        text_clean = text.strip().replace(" ", "")
        if pattern.fullmatch(text_clean):
            valid_texts.append(text_clean)
    return valid_texts

# -------------------------------------------------------------------
#                    Forklift and Plate Tracking
# -------------------------------------------------------------------
class ForkliftTracker:
    def __init__(self):
        self.forklift_data = {}
        self.plate_to_forklift = {}
        self.assigned_plates = set()  # Track assigned plates
        self.plate_detection_history = defaultdict(lambda: deque(maxlen=10))  # key: forklift_id, value: list of plate centers
        self.forklift_positions = defaultdict(lambda: deque(maxlen=10))  # Track forklift positions

    def set_speed_estimator(self, speed_estimator):
        self.speed_estimator = speed_estimator
    
    def get_bbox_hash(self, bbox):
        return hash(tuple(map(int, bbox)))
    
    def register_forklift(self, forklift_id, position):
        if forklift_id not in self.forklift_data:
            self.forklift_data[forklift_id] = {
                'plate_id': None,
                'position_history': deque(maxlen=10),
                'raw_texts': [],
                'confirmed_plate': False
            }
        self.forklift_data[forklift_id]['position_history'].append(position)
        self.forklift_positions[forklift_id].append(position)

    def is_plate_processed(self, plate_bbox):
        bbox_hash = self.get_bbox_hash(plate_bbox)
        return bbox_hash in self.assigned_plates
    
    def mark_plate_processed(self, plate_bbox):
        bbox_hash = self.get_bbox_hash(plate_bbox)
        self.assigned_plates.add(bbox_hash)
    
    def assign_plate_to_forklift(self, forklift_id, plate_number):
        if (forklift_id in self.forklift_data and 
            self.forklift_data[forklift_id]['plate_id'] is None and 
            plate_number not in self.plate_to_forklift):
            self.forklift_data[forklift_id]['plate_id'] = plate_number
            self.plate_to_forklift[plate_number] = forklift_id
            self.forklift_data[forklift_id]['confirmed_plate'] = True
            self.forklift_data[forklift_id]['raw_texts'] = []
            return True
        return False
    
    def get_forklift_plate(self, forklift_id):
        if forklift_id in self.forklift_data:
            return self.forklift_data[forklift_id]['plate_id']
        return None
    
    def add_raw_text_to_forklift(self, forklift_id, text):
        if forklift_id in self.forklift_data:
            self.forklift_data[forklift_id]['raw_texts'].append(text)
    
    def get_raw_texts_for_forklift(self, forklift_id):
        if forklift_id in self.forklift_data:
            return self.forklift_data[forklift_id]['raw_texts']
        return []
    
    def get_forklift_speed(self, forklift_id):
        if hasattr(self, 'speed_estimator') and self.speed_estimator:
            return self.speed_estimator.calculate_speed(forklift_id)
        return 0.0

    def update_plate_detection_history(self, forklift_id, plate_center):
        self.plate_detection_history[forklift_id].append(plate_center)

    def is_plate_near_previous_detection(self, plate_bbox, threshold=50):
        plate_center = np.array([(plate_bbox[0] + plate_bbox[2]) / 2, 
                                  (plate_bbox[1] + plate_bbox[3]) / 2])
        for _, history in self.plate_detection_history.items():
            if history:
                last_center = history[-1]
                if np.linalg.norm(plate_center - last_center) < threshold:
                    return True
        return False

# -------------------------------------------------------------------
#                       MAIN PROGRAM
# -------------------------------------------------------------------
def parse_arguments():
    parser = argparse.ArgumentParser(description="Forklift Speed + OCR with optimized tracking and speed estimation.")
    parser.add_argument("source_video_path", nargs="?", default=r"test3plate.mp4", help="Path to the source video file", type=str)
    parser.add_argument("target_video_path", nargs="?", default=r"test_output1.mp4", help="Path to the target video file (output)", type=str)
    parser.add_argument("--confidence_threshold", default=0.3, type=float, help="Confidence threshold for YOLO")
    parser.add_argument("--iou_threshold", default=0.7, type=float, help="IOU threshold for NMS")
    parser.add_argument("--target_fps", default=6.0, type=float, help="Target processing FPS (skip frames)")
    parser.add_argument("--debug", action="store_true", help="Display debug info, skip forklift polygon filter")
    return parser.parse_args()

# Plate detection polygon (for plates)
PLATE_POLYGON = np.array([[726, 466], [1255, 474], [1403, 1046], [600, 1057]], dtype=np.int32)

def is_point_in_polygon(px, py, polygon):
    return cv2.pointPolygonTest(polygon, (px, py), False) >= 0

def main():
    args = parse_arguments()
    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s: %(message)s")

    video_info = sv.VideoInfo.from_video_path(args.source_video_path)
    model = YOLO("best2.engine")

    # Setup the tracker
    byte_track = sv.ByteTrack(frame_rate=video_info.fps, track_activation_threshold=args.confidence_threshold)

    # Setup visualization
    thickness = sv.calculate_optimal_line_thickness(video_info.resolution_wh)
    text_scale = sv.calculate_optimal_text_scale(video_info.resolution_wh)
    box_annotator = sv.BoxAnnotator(thickness=thickness)
    label_annotator = sv.LabelAnnotator(text_scale=text_scale, text_thickness=thickness, text_position=sv.Position.BOTTOM_CENTER)
    trace_annotator = sv.TraceAnnotator(thickness=thickness, trace_length=int(video_info.fps) * 3, position=sv.Position.BOTTOM_CENTER)

    # Forklift detection zone and expanded polygon by 50 pixels left/right:
    SOURCE = np.array([[568, 310], [1321, 330], [1600, 1002], [264, 976]])
    min_x = np.min(SOURCE[:, 0])
    max_x = np.max(SOURCE[:, 0])
    mid_x = (min_x + max_x) / 2
    expanded_SOURCE = SOURCE.copy()
    for i in range(len(expanded_SOURCE)):
        if expanded_SOURCE[i, 0] < mid_x:
            expanded_SOURCE[i, 0] -= 50
        else:
            expanded_SOURCE[i, 0] += 50
    polygon_zone = sv.PolygonZone(polygon=expanded_SOURCE)

    # Real-world transform
    TARGET = np.array([[0, 0], [12.192, 0], [12.192, 12.192], [0, 12.192]])
    view_transformer = ViewTransformer(source=SOURCE, target=TARGET)

    # OCR setup
    reader = easyocr.Reader(['en'], gpu=True)
    ocr_pattern = re.compile(r'^(0[1-9]|[1-6]\d|70)$')

    # Initialize forklift tracker and speed estimator
    forklift_tracker = ForkliftTracker()
    speed_estimator = SpeedEstimator(args.target_fps, window_size=5)
    forklift_tracker.set_speed_estimator(speed_estimator)

    # Frame processing parameters
    skip_frames = max(int(video_info.fps / args.target_fps), 1)
    logging.info(f"Original FPS={video_info.fps}, target_fps={args.target_fps}, skip_frames={skip_frames}")

    # Replace frame_generator with cv2.VideoCapture for Jetson Nano compatibility
    cap = cv2.VideoCapture(args.source_video_path)
    frame_index = 0
    
    with sv.VideoSink(args.target_video_path, video_info) as sink:
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            frame_index += 1

            if frame_index % skip_frames != 0:
                sink.write_frame(frame)
                continue

            # Get frame dimensions
            height, width = frame.shape[:2]

            # Measure preprocess time (if any pre-processing is done; here it is a dummy op)
            pre_start = time.time()
            # ...no additional preprocessing...
            pre_time = (time.time() - pre_start) * 1000

            # Measure inference time
            inf_start = time.time()
            results = model(frame, verbose=False)[0]
            inf_time = (time.time() - inf_start) * 1000

            # Measure postprocess time for detection filtering, tracking etc.
            post_start = time.time()
            detections = sv.Detections.from_ultralytics(results)
            forklifts = detections[(detections.class_id == 0) & (detections.confidence > args.confidence_threshold)]
            plates = detections[(detections.class_id == 1) & (detections.confidence > args.confidence_threshold)]

            if not args.debug:
                forklifts = forklifts[polygon_zone.trigger(forklifts)]
            forklifts = forklifts.with_nms(threshold=args.iou_threshold)

            forklifts = byte_track.update_with_detections(detections=forklifts)
            forklift_centers = forklifts.get_anchors_coordinates(anchor=sv.Position.BOTTOM_CENTER)
            forklift_ids = forklifts.tracker_id

            real_world_centers = view_transformer.transform_points(forklift_centers)

            for i, f_id in enumerate(forklift_ids):
                forklift_tracker.register_forklift(f_id, forklift_centers[i])
                if i < len(forklift_centers) and i < len(real_world_centers):
                    speed_estimator.add_position(f_id, forklift_centers[i],
                        real_world_centers[i] if len(real_world_centers) > 0 else None)

            for j, plate_box in enumerate(plates.xyxy):
                px = (plate_box[0] + plate_box[2]) / 2
                py = (plate_box[1] + plate_box[3]) / 2

                if (not is_point_in_polygon(px, py, PLATE_POLYGON) or 
                    forklift_tracker.is_plate_processed(plate_box) or 
                    forklift_tracker.is_plate_near_previous_detection(plate_box)):
                    continue

                forklift_tracker.mark_plate_processed(plate_box)

                if len(forklift_centers) > 0:
                    plate_center = np.array([px, py])
                    distances = [np.linalg.norm(plate_center - fc) for fc in forklift_centers]
                    nearest_idx = np.argmin(distances)
                    nearest_forklift_id = forklift_ids[nearest_idx]

                    if forklift_tracker.forklift_data.get(nearest_forklift_id, {}).get('confirmed_plate', False):
                        continue

                    plate_crop = extract_plate(frame, plate_box)
                    if plate_crop is None or not quick_precheck(plate_crop, precheck_white_ratio=0.01):
                        continue

                    processed_plate = full_preprocess_pipeline(plate_crop)
                    base_angle = find_base_angle(processed_plate)
                    angle_offsets = [-10, -5, 0, 5, 10]
                    best_texts = []

                    for offset in angle_offsets:
                        test_angle = base_angle + offset
                        rotated_img = rotate_image(processed_plate, test_angle)
                        valid_texts = detect_two_digit_numbers(rotated_img, reader, ocr_pattern)
                        if len(valid_texts) > len(best_texts):
                            best_texts = valid_texts
                        if len(best_texts) >= 2:
                            break

                    for text in best_texts:
                        forklift_tracker.add_raw_text_to_forklift(nearest_forklift_id, text)

                    forklift_tracker.update_plate_detection_history(nearest_forklift_id, plate_center)

                    raw_texts = forklift_tracker.get_raw_texts_for_forklift(nearest_forklift_id)
                    if raw_texts:
                        freq_result = find_frequent_element(raw_texts)
                        if freq_result != 0:
                            assigned = forklift_tracker.assign_plate_to_forklift(nearest_forklift_id, freq_result)
                            if assigned:
                                logging.info(f"Forklift #{nearest_forklift_id} => Plate assigned: {freq_result}")

            labels = []
            for i, f_id in enumerate(forklift_ids):
                speed_mph = forklift_tracker.get_forklift_speed(f_id)
                plate_str = forklift_tracker.get_forklift_plate(f_id)
                if plate_str is not None:
                    lbl = f"Forklift #{f_id} Plate:{plate_str} Speed:{speed_mph:.1f} mph"
                else:
                    lbl = f"Forklift #{f_id} Speed:{speed_mph:.1f} mph"
                labels.append(lbl)

            post_time = (time.time() - post_start) * 1000
            # ...existing annotation code...
            annotated_frame = frame.copy()
            annotated_frame = trace_annotator.annotate(scene=annotated_frame, detections=forklifts)
            annotated_frame = box_annotator.annotate(scene=annotated_frame, detections=forklifts)
            annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=forklifts, labels=labels)

            # Compose overlay text as specified
            forklift_count = len(forklift_ids)
            plate_count = len(plates.xyxy)
            stop_count = 1  # (Using a dummy value)
            overlay_text_line1 = f"{frame_index}: {height}x{width} {forklift_count} forklift, {plate_count} plate, {stop_count} stop, {inf_time:.1f}ms"
            overlay_text_line2 = f"Speed: {pre_time:.1f}ms preprocess, {inf_time:.1f}ms inference, {post_time:.1f}ms postprocess per image at shape (1, 3, {height}, {width})"
            # Add terminal output of the overlay text
            
            print(overlay_text_line1)
            print(overlay_text_line2)
            
            sink.write_frame(annotated_frame)
            cv2.imshow("Forklift Tracking", annotated_frame)

            if cv2.waitKey(1) & 0xFF == ord("q"):
                break

    cap.release()
    cv2.destroyAllWindows()
    sys.exit(0)

if __name__ == "__main__":
    main()